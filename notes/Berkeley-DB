Berkeley BD study
+++++++++++++++++

This document contains study about Berkeley DB internals. Especially about consistency, durability and recovery.

Introduction - Storage engines and consistency protocol
+++++++++++++++++++++++++++++++++++++++++++++++++++++++

Berkeley DB uses different storage techniques internally to store the key value pairs like BTREE, HASH, QUEUE and RECNO. The organization of key value pairs in these datastructures are quite different. BTREE is the most commonly used type. There are different flags with respect to durability and consistency of the database. Berkeley DB supports both transactional API and non-transactional API to interact with the store. In the non-transactional API, there is no guarantee that the database will be consistent and corruption free. To enable transactional access to the database, an environment has to be initialized with proper flags. Depending on the values of these flags, the consistency and durability guarantees change. 

Basic protocol to ensure consistency : Write ahead logging (WAL). 

The transaction data going to the log file is protected by checksum. ie., when the system or the application crashes when writing the log file, the next open of the environment can detect that the last transaction had an incomplete write to the log file and can ignore to checkpoint this transaction. 

Flags pertaining to durability and consistency:

DB_TXN_SYNC 
DB_TXN_NOSYNC  
DB_TXN_WRITE_NOSYNC

DB_TXN_SYNC - Synchronously flush the log when this transaction commits. This means the transaction will exhibit all of the ACID (atomicity, consistency, isolation, and durability) properties.

DB_TXN_NOSYNC - Do not synchronously flush the log when this transaction commits or prepares. This means the transaction will exhibit the ACI (atomicity, consistency, and isolation) properties, but not D (durability); that is, database integrity will be maintained but it is possible that this transaction may be undone during recovery. This will not give durability in case of operating system and application crash.

DB_TXN_WRITE_NOSYNC - Berkeley uses 2 level caching. One that is internal to the application and the other is the operating system buffer cache. Write (from the application buffer to OS buffer cache), but do not synchronously flush, the log when this transaction commits. This means the transaction will exhibit the ACI (atomicity, consistency, and isolation) properties, but not D (durability); that is, database integrity will be maintained, but if the system fails, it is possible some number of the most recently committed transactions may be undone during recovery. The number of transactions at risk is governed by how often the system flushes dirty buffers to disk and how often the log is flushed or checkpointed. Note: This flag will give durability in case of application crash.

In all the three cases, the database integrity should not be relaxed. 

WAL - It is a typical WAL protocol. The changes are first written to the log file using write system call and then a fdatasync on the log is called. The amount of data going to the log file is proportional to the data being written as part of the transaction. So this write can be split into many calls. It is possible that a crash can happen in the middle of the log writing. Berkeley DB handles this using checksums (Even in 'out of order writing' file systems? - I am not sure about this. A similar bug has been filed on SQLite - http://www.sqlite.org/src/info/ff5be73dee. Thanu, please confirm.)  

The log syncing varies by the above 3 flags. 

After writing to the log and syncing it (depending on the flag), the datafile is written and a sync is done after writing. This write to the datafile can happen at various points. Below are some of them:

1. A close call on the database will flush all the datapages.
2. An explicit sync call on the database.

The write to the data file can span many pages and hence file blocks. A crash in the middle of these writes can have different effects depending on when the crash happens. Berkeley DB documentation mentions about the atomic page writes requirement - see http://docs.oracle.com/cd/E17275_01/html/programmer_reference/transapp_reclimit.html.  By this requirement, the application has to be configured to use the same write block size used by the operating system. For example, if the OS block size is 4K, then DB has to be also configured to write in 4K chunks. 

A crash that happens in middle of writing a datafile can cause corruption to the datafile. To catch this corruption, the database has to be configured with DB_CHKSUM when opening the database. By default, the datafile is not protected by checksums. If there was an interruption while writing to the datafile, next time open with DB_RECOVER flag can detect this and replay the transaction to the datafile from the log.

Workloads and experiments:
++++++++++++++++++++++++++

Experiments were run with following configurations:

Dimensions considered: STORAGE ENGINE, DURABILITY FLAGS, TXN SIZE

This is the cartesian product representation of the test cases : {BTREE,HASH,QUEUE,RECNO} X {SYNC,NOSYNC,WRITE_NOSYNC} X {SMALL TXN, LARGE TXN}

SMALL TRANSACTIONS - Involves putting 10 k,v pairs as part of one transaction:

SYNC - BTREE = /*
NOSYNC - BTREE = /+*
WRITENOSYNC - BTREE = /+*

SYNC - HASH = /*
NOSYNC - HASH = /+*
WRITENOSYNC - HASH = /+*

SYNC - QUEUE = /
NOSYNC - QUEUE = /+*
WRITENOSYNC - QUEUE = /+*

SYNC - RECNO = /*
NOSYNC - RECNO = /+*
WRITENOSYNC - RECNO = /+*


LARGE TRANSACTIONS - Involves putting 1000 k,v pairs as part of one transaction - This involves log switching.

SYNC - BTREE = /*
NOSYNC - BTREE = /+*
WRITENOSYNC - BTREE = /+*

SYNC - HASH = /*
NOSYNC - HASH = /+*
WRITENOSYNC - HASH = /+*

SYNC - QUEUE = /*
NOSYNC - QUEUE = /+*
WRITENOSYNC - QUEUE = /+*

SYNC - RECNO = /*
NOSYNC - RECNO = /+*
WRITENOSYNC - RECNO = /+*

LEGEND  
+++++++

/ - Proper recovery 
+ - Proper warning in documentation about durability
* - Means a page unaligned crash when writing datafile fails to recover though the log file is persistent. ie., the log file is persisted and a crash happens when the data file is being written. The crash happens in such a way that it is not page aligned. Thought the DB is not configured with checksum, they could have replayed this from the log. This could be prevented if checksums are enabled for the database and fatal recovery (DB_RECOVER_FATAL) is done. 


Large transactions - Inserting 1000 values in one transaction - This is to test the scenario where the log page written to the disk is more than the OS block size. BSDDB devs seem to be addressing this issue properly in their documentation - See this link on atomic page writes - http://docs.oracle.com/cd/E17275_01/html/programmer_reference/transapp_reclimit.html.

Log file switching - This is to test when the log data to be written exceeds the configured size (For example 10 * 4096 Bytes = 10 pages) number of bytes. In this case, BDB starts creating new log files. The old files as they are getting filled are written and *flushed* to disk. When the transaction commits, the last in-memory log file is flushed to disk.
Tests for this included crashing in middle at various points - They seem to be handling these scenarios fairly well!! (use of checksums).

Apart from the above test matrix, few more experiments were run:

1. Multiple transactions - Berkeley DB uses same set of techniques to ensure integrity (Tested for all storage engines).
2. Remove an intermediate log file after running a complete transactional workload just to see what happens - In this case, the environment open call fails with an error saying that it is not able to find the environment. 

Vulnerabilities:
+++++++++++++++

1. Safe new file flush assumption on database file.
2. Refer * in legend.
3. This is a documented corruption bug - http://docs.oracle.com/cd/E17076_03/html/programmer_reference/transapp_journal.html. It is possible for the log file to get corrupted. The window of vulnerability is very small though. 

Note about data persistence in Berkeley DB documentation:
++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Note that as a normal part of closing a database, its cache is written to disk. However, in the
event of an application or system failure, there is no guarantee that your databases will close
cleanly. In this event, it is possible for you to lose data. Under extremely rare circumstances,
it is also possible for you to experience database corruption.

Also this is mentioned:- But we have to check if txn commit actually flushes data to disk.

Therefore, if you care if your data is durable across system failures, and to guard against the
rare possibility of database corruption, you should use transactions to protect your database
modifications. Every time you commit a transaction, DB ensures that the data will not be lost
due to application or system failure. Transaction usage is described in the Berkeley DB Getting
Started with Transaction Processing guide.

If you do not want to use transactions, then the assumption is that your data is of a nature
that it need not exist the next time your application starts. You may want this if, for example,
you are using DB to cache data relevant only to the current application runtime.Library Version 12.1.6.0 Database Records
5/31/2013 Getting Started with DB Page 26
If, however, you are not using transactions for some reason and you still want some guarantee
that your database modifications are persistent, then you should periodically call DB-
>sync(). Syncs cause any dirty entries in the in-memory cache and the operating system's
file cache to be written to disk. As such, they are quite expensive and you should use them
sparingly.

Remember that by default a sync is performed any time a non-transactional database is closed
cleanly. (You can override this behavior by specifying DB_NOSYNC on the call to DB->close().)
That said, you can manually run a sync by calling DB->sync().

NOTE: If your application or system crashes and you are not using transactions, then you
should either discard and recreate your databases, or verify them. You can verify a
database using DB->verify(). If your databases do not verify cleanly, use the db_dump
command to salvage as much of the database as is possible. Use either the -R or -r
command line options to control how aggressive db_dump should be when salvaging
your databases.


Interesting code comments:
++++++++++++++++++++++++++

/*
 * The transactional guarantees Berkeley DB provides for file
 * system level operations (database physical file create, delete,
 * rename) are based on our understanding of current file system
 * semantics; a system that does not provide these semantics and
 * guarantees could be in danger.
 *
 * First, as in standard database changes, fsync and fdatasync must
 * work: when applied to the log file, the records written into the
 * log must be transferred to stable storage.
 *
 * Second, it must not be possible for the log file to be removed
 * without previous file system level operations being flushed to
 * stable storage.  Berkeley DB applications write log records
 * describing file system operations into the log, then perform the
 * file system operation, then commit the enclosing transaction
 * (which flushes the log file to stable storage).  Subsequently,
 * a database environment checkpoint may make it possible for the
 * application to remove the log file containing the record of the
 * file system operation.  DB's transactional guarantees for file
 * system operations require the log file removal not succeed until
 * all previous filesystem operations have been flushed to stable
 * storage.  In other words, the flush of the log file, or the
 * removal of the log file, must block until all previous
 * filesystem operations have been flushed to stable storage.  This
 * semantic is not, as far as we know, required by any existing
 * standards document, but we have never seen a filesystem where
 * it does not apply.
 */