LMDB study
+++++++++++

This doc contains ALC study for Lightning Memory-mapped Database.

Introduction and data structures:
++++++++++++++++++++++++++++++++

LMDB uses a append-only B+ tree structure which does not need any kind of compaction or garbage collection. No application-level caching is required: LMDB relies entirely on the operating systemâ€™s buffer cache. Memory mapped, allowing for zero copy get operations. 

In a MDB database environment, there are two B+tree structures - one containing application data and another that contains a free list with the IDs of pages that are no longer in use. The updaters can use the free pages when they find one and so the database size remains constant more or less. 

A meta page that is found in the beginning of the database file points to the roots of the above 2 tree structures. There are 2 such meta pages and transactions alternate between them for metadata updates. Specifically, even numbered transactions use page 0 and odd numbered transactions use page 1. No multi-writers for an environment is supported. The metadata is usually 106 bytes in length. LMDB has only transactional API - i.e., all put and get operations can happen through a transactional interface only. 

Basic flow:
+++++++++++

PUT: When a new key value pair is inserted, a new data page (or a free one) is used and written at the end of the file. Corresponding changes to the structure of the B+ tree are made in memory and then persisted. After this, a fdatasync() is called on the file (data.mdb). After this sync operation, the metadata is written. 

***IMPORTANT*** : There are 2 file descriptors for the data file - FD1 and FD2. FD2 is a sync FD - i.e., it has O_DSYNC flag set in it. So any write operation that uses FD2 will be immediately range-synced to the medium.  

The data region of the file is written using FD1 whereas the metapage updates are done through FD2. 

If there is a crash between writing the data and the metadata, metapage would point to the older version of the database and hence be consistent. If the metadata write happened properly, the database state would be updated and it would be in the newer state. 

KEY ASSUMPTION : Metadata write (106 bytes) is atomic. We have experimented breaking this assumption - i.e., What happens if this write is not atomic. 

Features and configurations:
++++++++++++++++++++++++++++

As mentioned earlier, the only way to interact with the data store is through a transactional interface. Usually normal transactional operations are synchronous. i.e., Whenever a transaction is commited, all the data and metadata are synchronized to the disk. But there are few configuration options that can change this behavior. 

These configuration options apply at the environment level only and not for a particular transaction:

'metasync' : If False, never explicitly flushes metadata pages to disk. OS will flush at its discretion, or user can flush with sync().
'sync': If False, never explicitly flush data pages to disk. OS will flush at its discretion, or user can flush with sync(). This optimization means a system crash can corrupt the database or lose the last transactions if buffers are not yet flushed to disk.
'writemap': If True LMDB will use a writeable memory map to update the database. This option is incompatible with nested transactions.
'map_async': When writemap=True, use asynchronous flushes to disk. As with sync=False, a system crash can then corrupt the database or lose the last transactions. Calling sync() ensures on-disk database integrity until next commit.

Default configuration: writemap = False, metasync= True, map_async doesn't matter. 

Note that there is a clear mention about the integrity of the DB if some of the options are turned off or on. Also note that there is no proper mention about what happens to integrity when only metasync is set to false.  These above configurations change the strace collected for the workloads in a significant way. 

Workloads:
++++++++++

Following workloads were created to understand the internals of LMDB.

1. Simple put-get tests with single and multi transactions.
2. Nested transactions.
3. Simple value replace. 
4. Simple key delete.
5. Simple opening of environment.
6. Simple opening of named DB.
7. Simple update. 

All these experiments were run with legitimate but crashed versions of data file. 

Results:
++++++++

Following are the vulnerabilities that were discovered in LMDB.

Vulnerabilities:
===============

1. Safe new file flush assumption : LMDB assumes safe new file flush property. ''' Copied from ALC paper: This property asserts that, the post-crash state contains the directory entry corresponding to the created file. Some Linux manpages [10] for fsync() explicitly warn that this property should not be assumed, and that a fsync() on the directory itself is required for guaranteed existence of the directory entry. '''

LMDB creates 2 files (data.mdb and lock.mdb) when an environment is opened but does not do a sync on the parent directory to ensure that directory entries for the created files are persisted. This is just needed for the first time when the environment is created as all subsequent created DBs go into the same file. But this is not done.   

2. Process crash when opening env : As mentioned earlier, LMDB writes two metapages to the beginning of the data file. There is no sync call made after writing these 2 pages. A crash can happen before these pages are persisted to disk. If this crash happens, any subsequent open calls on the same environment will fail with an error saying that the data file is not a valid MDB file. Note: This bug is not related to FS portability. This is a generic bug in terms of consistency.

3. Splitting 106 bytes of metadata: The metadata is always written using FD2(the one with O_DSYNC flag). But there is a chance that this write can be split and a crash can happen in middle. There are actually two important parts in the metadata written - root of the data B+tree and the root of the free pages list. If a crash happened in such a way that the free page pointer doesn't reach the disk but the root of the data B+tree reaches the disk, then subsequent put operations will fail as the data file is in a corrupted state.

Error that is seen for this condition: 

lib/mdb.c:1921: mdb_page_touch: Assertion `mp->mp_p.p_pgno != pgno' failed

There is an invariant always expected by mdb => free_pages + current_pages + 2 metapages = in_use_pages - 1. The above crash violates this expectation and hence the problem occurs.  

Note: If the free pages section reaches the disk but not the root of the data B+tree, there is no problem. 

Perfomance numbers for various sync configurations:
+++++++++++++++++++++++++++++++++++++++++++++++++++

Workload details: Insert 500 key value pairs with 500 txns. 

Configurations and elapsed time in seconds:

Default configuration: writemap = False, metasync= True, map_async doesn't matter.

1. Default: 10.9678471088, 8.94234490395, 13.0473058224 , 9.11274194717, 13.341780901  ---- Average: 11.0824041367

2. metasync = False: 8.55708885193, 10.4846498966, 9.51322102547, 12.2384150028, 10.535161972  ---- Average: 10.2657073498

3. metasync = True, writemap = True, map_async = False: 29.6565160751, 29.710395813, 29.710395813, 30.6400279999, 29.4861288071 ---- Average: 29.8406929016

4. metasync = False, writemap = True, map_async = True: 0.00295209884644, 0.00280785560608, 0.00282120704651, 0.00293898582458, 0.00282692909241 ---- Average: 0.0028694152832

5. metasync = True, writemap = True, map_async = True: 0.00293183326721, 0.00306487083435, 0.00297808647156, 0.00288105010986, 0.00291013717651 ---- Average: 0.0029531955719

6. sync = False: 0.00669407844543, 0.00664901733398, 0.00672101974487, 0.00664591789246, 0.00693702697754 ---- Average: 0.00672941207886


Splitting metadata and crashing:
++++++++++++++++++++++++++++++++

Metadata total size : 106 bytes.

This is the memory layout of the metadata:

1. First 42 bytes for free list related data.
2. Next 48 bytes for database related data
(pad-4,flags-2,depth-2,branch_page_count-8,leaf_count-8,overflow_page_count-8,entries-8,root_of_tree-8)
3. Last_page_in_the_tree - 8 bytes.
4. Transaction ID - 8 bytes.

Note: 42+48+8+8 == 106.

Initial setup: There were 3 previous transactions on the database. The
workload is the 4th txn on the database.

I tried the following crash scenarios with respect to metadata:

1. Only txn_id gets to disk
2. Free list data, txn_id and last_page gets to disk but not main db details
3. Only the last_page field goes to disk
4. only main db + 2 fields go to disk and free pointer is missed
5. only free pointer goes to disk
6. Everything gets in except complete database pointer - it is 48 bytes -
but a crash happens such that second 24 bytes (which contains the root) is
missed
7. Everything gets in except complete database pointer - it is 48 bytes -
but a crash happens such that first 24 bytes (which contains depth,
br_pages, leaves count etc) is missed

After crashing, the query was made for all key value pairs and a new txn to
insert a new key value pair was tried.

Results:
++++++++

1. Only txn_id gets into metapage and no other data:

Values obtained state: txn2
Result : No issues.

2. Free list data, txn_id and last_page goes in but not main db details:

Values obtained state: txn2
Result : Error: lib/mdb.c:1921: mdb_page_touch: Assertion `mp->mp_p.p_pgno
!= pgno' failed.

3. Only the last_page field goes in nothing else:

Values obtained state: txn3
Result : No issues.

4. only main db + 2 fields no free pointer:

Values obtained state: txn4
Result : Error: lib/mdb.c:1921: mdb_page_touch: Assertion `mp->mp_p.p_pgno
!= pgno' failed.

5. only free pointer goes to disk:

Values obtained state : txn3
Result : No issues.

6. Everything gets in except database pointer - it is 48 bytes - but a crash
happens such that second 24 bytes (which contains the root) is missed:

Values obtained state: txn2
Result : Error: lib/mdb.c:1921: mdb_page_touch: Assertion `mp->mp_p.p_pgno
!= pgno' failed.

7. Everything gets in except database pointer - it is 48 bytes - but a crash
happens such that first 24 bytes (which contains depth, br_pages, leaves
count etc) is missed:

Values obtained state: txn4
Result : No issues.

Conclusion: As we can see, the database is always consistent with key value
pairs. But the internal consistency of the database is not maintained which
is revealed by subsequent operations. As already mentioned, the invariant
free_pages + current_pages + 2 metapages = in_use_pages -1, does not hold in
some cases after crashes and that is the reason for all seen errors.